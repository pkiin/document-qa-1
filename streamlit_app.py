import streamlit as st
from openai import OpenAI
from langchain_community.document_loaders import PyPDFLoader
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain import hub
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_openai.chat_models import ChatOpenAI
from langchain.schema import Document
import tempfile
import os

global filename_cache

# Show title and description.
st.title("ğŸ“„ Document question answering")
st.write(
    "Upload a document below and ask a question about it â€“ GPT will answer! "
    "To use this app, you need to provide an OpenAI API key, which you can get [here](https://platform.openai.com/account/api-keys). "
)

# Ask user for their OpenAI API key via `st.text_input`.
# Alternatively, you can store the API key in `./.streamlit/secrets.toml` and access it
# via `st.secrets`, see https://docs.streamlit.io/develop/concepts/connections/secrets-management
openai_api_key = st.text_input("OpenAI API Key", type="password")
if not openai_api_key:
    st.info("Please add your OpenAI API key to continue.", icon="ğŸ—ï¸")
else:

    filename_cache=''

    # Set the API key for LangChain components
    os.environ["OPENAI_API_KEY"] = openai_api_key

    # Let the user upload a file via `st.file_uploader`.
    uploaded_file = st.file_uploader(
        "Upload a document (.txt or .md or .pdf)", type=("txt", "md","pdf")
    )

    # Ask the user for a question via `st.text_area`.
    question = st.text_area(
        "Now ask a question about the document!",
        placeholder="Can you give me a short summary?",
        disabled=not uploaded_file,
    )

    if uploaded_file and question:

        if(filename_cache!=uploaded_file.name):
            # íŒŒì¼ í™•ì¥ìë¡œ íƒ€ì… í™•ì¸
            file_extension = uploaded_file.name.split('.')[-1].lower()

            if file_extension == "pdf":
                # PDF íŒŒì¼ì„ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                    tmp_file.write(uploaded_file.getvalue())
                    tmp_file_path = tmp_file.name
                
                try:
                    # PyPDFLoaderë¡œ PDF ë¡œë“œ
                    loader = PyPDFLoader(tmp_file_path)
                    docs = loader.load()
                    
                finally:
                    # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                    os.unlink(tmp_file_path)
            else:
                document = uploaded_file.read().decode()
                # Document ê°ì²´ë¡œ ë³€í™˜
                docs = [Document(page_content=document)]

            # ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• 
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200
            )
            split_docs = text_splitter.split_documents(docs)
            
            # ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„±
            with st.spinner("ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                embeddings = OpenAIEmbeddings()
                vectordb = FAISS.from_documents(split_docs, embeddings)
            
                # ì§ˆì˜ì‘ë‹µ ì²´ì¸ êµ¬ì„±
                try:
                    # LangChain Hubì—ì„œ RAG í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
                    prompt = hub.pull("rlm/rag-prompt")
                except:
                    # Hubì—ì„œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ì‹œ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
                    from langchain_core.prompts import ChatPromptTemplate
                    prompt = ChatPromptTemplate.from_template("""
                    You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.

                    Question: {question} 

                    Context: {context} 

                    Answer:
                    """)
                
                def format_docs(docs):
                    return "\n\n".join(doc.page_content for doc in docs)
                
                # ê²€ìƒ‰ê¸° ì„¤ì •
                retriever = vectordb.as_retriever(search_kwargs={"k": 10})
        else:
            filename_cache=uploaded_file.name
        # ì§ˆì˜ì‘ë‹µ ì²´ì¸ êµ¬ì„±
        qa_chain = (
            {
                "context": retriever | format_docs,
                "question": RunnablePassthrough(),
            }
            | prompt
            | ChatOpenAI(model='gpt-4o-mini')
            | StrOutputParser()
        )

        qa_chain_with_context = (
            RunnableParallel(
                {
                    "context": retriever,
                    "question": RunnablePassthrough(),
                    "answer": (  # LLMì˜ ê²°ê³¼ë¥¼ "answer" í‚¤ë¡œ ì €ì¥
                        {
                            "context": retriever | format_docs,
                            "question": RunnablePassthrough(),
                        }
                        | prompt
                        | ChatOpenAI(model='gpt-4o-mini')
                        | StrOutputParser()
                    ),
                }
            )
        )
        
        # ì§ˆë¬¸ ì‹¤í–‰
        with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            result = qa_chain_with_context.invoke(question)
        
        # ë‹µë³€ í‘œì‹œ
        st.subheader("ë‹µë³€:")
        st.write(result['answer'])

        
        # ì†ŒìŠ¤ ë¬¸ì„œ í‘œì‹œ
        st.subheader("ì°¸ì¡°ëœ ë¬¸ì„œ ì²­í¬:")
        for i, doc in enumerate(result["context"]):
            with st.expander(f"ì²­í¬ {i+1}"):
                st.write(doc.page_content)
                # ë©”íƒ€ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš° í‘œì‹œ
                if doc.metadata:
                    st.write("**ë©”íƒ€ë°ì´í„°:**")
                    st.json(doc.metadata)
